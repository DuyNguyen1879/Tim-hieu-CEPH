Thuật toán CRUSH quyết làm thế nào để lưu trữ và khôi phục data bằng cách tính toán data location

### CRUSH Location

Vị trí của OSD trong trong bản đồ phân cấp CRUSH được gọi là **crush location**. Vị trí này có dạng key và value để mô tả vị trí. Ví dụ nếu một OSD cụ thể nằm trong row, rack, chassis và host là một phần của "default" CRUSH tree, crush location sẽ được mô tả như sau:

``root=default row=a rack=a2 chassis=a2a host=a2a1``

Note:

- Thứ tự các key không quan trọng
- Key name (bên trái =) phải là một CRUSH type hợp lệ. Mặc định gồm :  root, datacenter, room, row, pod, pdu, rack, chassis và host
- Không cần xác định tất cả các key. Ví dụ, mặc định thì Ceph sẽ set **ceph-osd** location ở ``root=default host=HOSTNAME``

crush location cho OSD thường được thể hiện qua **crush location** option được set trong ``ceph.conf``. Mỗi khi OSD start, nó sẽ xác minh nó ở đúng vị trí trong CRUSH map, nếu không nó sẽ tự di chuyển chính nó. Để vô hiệu hóa việc tự động quản lý CRUSH map, thêm dòng sau vào config file trong phần [OSD]

``osd crush update on start = false``

### CRUSH STRUCTURE

CRUSH map gồm một hệ thống phân cấp mô tả topology vật lý của cluster và rules quy định policy về cách lưu trữ data trên thiết bị. 

#### Device

Device là ``ceph-osd`` daemon có thể lưu trữ data. Device được xác định bởi một id và tên thường là osd.N trong đó N là device id

Device cũng có ``device class`` được gắn liền với chúng (ví dụ: hdd hoặc ssd)

#### Types và buckets

Buckets là thuật ngữ cho các node trong bản đồ phân cấp như: hosts, racks, rows, ... CRUSH map định nghĩa các type được dùng để mô tả các node. Mặc định các type bao gồm: osd (hoặc device), host, chassis, rack, row, pdu, pod, room, datacenter, region, root,...

Mỗi node(device hoặc bucket) trong hệ thống phân cấp có chỉ số ``weight`` cho thấy tỷ lệ tương đối của tổng số dữ liệu mà thiết bị hoặc hệ thống phân cấp có thể lưu trữ

#### Rules

Rules quy định policy về cách dữ liệu được phân phối trên các devices trong hệ thống phân cấp

CRUSH rules xác định vị trí và và replication hoặc distribution policy cho phép ta xác định chính xác nơi CRUSH phân phối các bản sao.

Hầu hết ta có thể tạo CRUSH rules thông qua CLI bằng cách xác định ``pool type`` sẽ được sử dụng cho replicated hoặc erasure coded, ``failure domain`` và tùy chọn ``device class``. 

Ta có thể xem các rule được xác định cho cluster:

``ceph osd crush rule ls``

Ta có thể xem nội dung của rule:

``ceph osd crush rule dump``

#### Device classes

Mỗi device có một tùy chọn ``class`` gắn với nó. Mặc định, OSDs tự động thiết lập class của chúng khi khởi động có thể là: hdd, ssd hoặc nvme dựa trên4

### Modifying the crush map

#### Add/move OSD

Để add hoặc di chuyển OSD trong CRUSH map của một cluster đang chạy

``ceph osd crush set {name} {weight} root={root} [{bucket-type}={bucket-name} ...]``

Ví dụ add osd.0 vào hệ thống phân cấp hoặc di chuyển OSD

``ceph osd crush set osd.0 1.0 root=default datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1``

#### Điều chỉnh OSD Weight

Để chiều chỉnh crush weight của OSD trong CRUSH map thực thi câu lệnh:

``ceph osd crush reweight {name} {weight}``

#### Remove OSD

``ceph osd crush remove {name}``

#### Add a bucket

Để add backet vào CRUSH map thực thi câu lệnh ``ceph osd crush add-bucket``

``ceph osd crush add-bucket {bucket-name} {bucket-type}``

#### Di chuyển bucket

``ceph osd crush move {bucket-name} {bucket-type}={bucket-name}, [...]``

#### Xóa bucket

``ceph osd crush remove {bucket-name}``

#### Create rule cho replicated pool

Với replicated pool, công việc chính khi tạo CRUSH rule là xác định failure domain. Ví dụ, nếu failure domain của host được chọn, sau đó CRUSH sẽ chắc chắn rằng mỗi bản repilca của data sẽ được lưu trữ tại các host khác nhau. Nếu rack được chọn, mỗi bản replica sẽ được lưu trữ trên rack khác nhau.

Nó cũng có thể tạo rule để giới hạn vị trí dữ liệu đến một ``class`` của device. Mặc định, Ceph OSDs tự động gán là hdd hay ssd tùy thuộc vào thiết bị đang sử dụng

Để tạo replicate rule:

``ceph osd crush rule create-replicated {name} {root} {failure-domain-type} [{class}]``

#### Create rule erasure coded pool

Đối với erasure-coded pool, việc tạo pool cũng gần giống so với replicated pool. Erasure code pool được tạo có một chút khác biệt, tuy nhiên, bởi vì họ cần xây dựng cẩn thận dựa trên erasure code được sử dụng. Vì vậy, ta cần tông hợp thông tin erasure code vào trong ``erasure code profile``. CRUSH rule và sẽ được sử dụng khi tạo ra một pool

Erasure code profile có thể list với câu lệnh:

``ceph osd erasure-code-profile ls``

Xem các profile đang có sẵn:

``ceph osd erasure-code-profile get {profile-name}``

Một erasure code profile bao gồm các cặp key=value. Các tính chất đáng chú ý:

- **crush-root**: tên của CRUSH node để lưu trữ data (mặc định: default)
- **crush-failure-domain**: CRUSH type để lưu trữ các mảnh erasure-coded (mặc định: host)
- **crush-device-class**: device class để lưu trữ dữ liệu (default: none)
- **k** và **m**: xác định số lượng của mảnh erasure code, 

Khi profile được xác định, ta có thể tạo CRUSH rule:

``ceph osd crush rule create-erasure {name} {profile-name}``

#### Delete rules

``ceph osd crush rule rm {rule-name}``


